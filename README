Генератор карты сайтов
В карту сайтов попадает страница, отвечающая на запрос GET кодом 200 с
заголовком 'text/html'. Наполнение производится за счет парсинга html разметки
на элементы с аттрибутом href, которые в свою очередь попадают в очередь
на парсинг.
Тест проводился на Linux Mint с запуском строго из командной строки.
Результат тестов представлен в файле results.txt
Использована мультипоточная и мультипроцессорная обработка средствами
concurrent.futures.ThreadPoolExecutor и multiprocessing.Process соответственно.

Критические места, которых, увы, довольно много:
    1) Использование словаря для хранения имен страниц, по которым в дальнейшем
    будет выполняться поиск, дабы избежать повторов посещенных адресов.
    Решением было бы использование файлоподобных объектов, однако на тестах время
    тестирования каждой урлки превысило все разумные пределы при парсинге сайтов
    с большим количеством страниц.
    2) Логическим продолжением первого пункта служит ограничение количества
    найденных страниц, которое явилось строгой необходимостью, т.к. моя система не выдерживает
    нагрузки при парсинге больших сайтов.
    3) Еще одним сомнительным решением служит ограничение страниц, чей родительский адрес
    уже встречался более N раз (в моём случае 300). Число N бралось с потолка.
    Целью этой особенности является выход из злонамеренной генерации бесконечного числа
    страниц с уникальными адресами. Я понимаю, что это не панацея и во многих случаях
    парсинга больших сайтов приносит больше вреда, чем пользы, однако как-то выходить из
    бесконечного цикла все же нужно. И хотя можно было бы ограничиться просто пунктом два и
    ждать, пока словарь не заполнится, все же это решение было реализовано в целях экономии времени
    анализа сайта.
    4) Анализируется только html разметка, другие способы генерации url не
    учитываются.
    5) Скорость анализа, конечно, довольно медленная
    6) При анализе сайтов, чьё количество страниц больше установленного ограничения,
    пул потоков не позволяет сразу остановится и продолжает анализ ещё какое-то время после
    заполнения словаря. Не уверен, что это проблема большая, чем само установленное ограничение,
    ибо само ограничение сводит пользу от программы на "нет".
Решением нескольких из вышеописанных проблем было бы использование базы данных и пренебрежение
временем обработки. Однако, пока что так.

Тем не менее, программа подходит для анализа сайтов, чьё количество страниц
поддаётся исчеслению.
